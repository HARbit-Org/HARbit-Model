{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d80f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder, \n",
    "    StandardScaler,\n",
    "    label_binarize\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Save variables for model\n",
    "import joblib\n",
    "\n",
    "# PERSONAL FUNCTIONS\n",
    "from utils import *\n",
    "from models.main import *\n",
    "from models.optimizer import ViterbiLiteDecoder\n",
    "from functions.windows import create_feature_windows # creación de ventanas e ingenieria de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultados = { 'Sit': [], 'Stand': [], 'Type': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564fa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0.0),\n",
       " (0.11764705882352941, 0.0, -0.11764705882352941),\n",
       " (0.058823529411764705, 0.0, -0.058823529411764705),\n",
       " (0.0, 0.0, 0.0),\n",
       " (0.0, 0.0, 0.0),\n",
       " (0.0, 0.0, 0.0),\n",
       " (0.0, 0.0, 0.0),\n",
       " (0.0, 0.0, 0.0),\n",
       " (0.0, 0.0, 0.0)]"
      ]
     },
     "execution_count": 1236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados[target][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sit': [(0.08333333333333333, 0.0, -0.08333333333333333),\n",
       "  (0.058823529411764705, 0.0, -0.058823529411764705),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.25, 0.25, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.25, 0.2916666666666667, 0.041666666666666685),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0)],\n",
       " 'Stand': [(0.14285714285714285, 0.19047619047619047, 0.047619047619047616),\n",
       "  (0.06666666666666667, 0.0, -0.06666666666666667),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.125, 0.125, 0.0),\n",
       "  (0.058823529411764705, 0.0, -0.058823529411764705),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.5333333333333333, 0.5333333333333333, 0.0),\n",
       "  (0.11764705882352941, 0.0, -0.11764705882352941)],\n",
       " 'Type': [(0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.11764705882352941, 0.0, -0.11764705882352941),\n",
       "  (0.058823529411764705, 0.0, -0.058823529411764705),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0),\n",
       "  (0.0, 0.0, 0.0)]}"
      ]
     },
     "execution_count": 1499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "id": "447eabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = \"09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "id": "1a2f7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"F:\\UPC\\Tesis\\HARbit-Model\\src\\data\\real-data\\type-data\\type_left_04.json\"\n",
    "data = r\"type_left_04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "id": "5ddc8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "gyro_df = data['gyro']\n",
    "accel_df = data['accel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "id": "8d949116",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "id": "3e6157e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_temp = pl.DataFrame(accel_df)\n",
    "gyro_temp = pl.DataFrame(gyro_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "id": "8b3ea5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_temp = accel_temp.with_columns(pl.lit('A').alias('Usuario'))\n",
    "gyro_temp  = gyro_temp.with_columns(pl.lit('A').alias('Usuario'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1527,
   "id": "7fb43d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_temp   = accel_temp.with_columns(pl.lit(target).alias('gt'))\n",
    "gyro_temp    = gyro_temp.with_columns(pl.lit(target).alias('gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "id": "dce7a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel = normalize_columns(accel_temp,\n",
    "                            user_col_name  = \"Usuario\", \n",
    "                            timestamp_col_name = \"timestamp\", \n",
    "                            label_col_name = \"gt\", \n",
    "                            x_col_name = \"x\", \n",
    "                            y_col_name = \"y\", \n",
    "                            z_col_name = \"z\")\n",
    "\n",
    "df_gyro = normalize_columns(gyro_temp, \n",
    "                            user_col_name  = \"Usuario\", \n",
    "                            timestamp_col_name = \"timestamp\", \n",
    "                            label_col_name = \"gt\", \n",
    "                            x_col_name = \"x\", \n",
    "                            y_col_name = \"y\", \n",
    "                            z_col_name = \"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "id": "64cea588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_sensors = df_gyro.join(df_accel, on = ['Subject-id', 'Timestamp', 'Activity Label'], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "id": "cfcd2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gyro = df_all_sensors.select([\n",
    "#     'Subject-id',\n",
    "#     'Timestamp',\n",
    "#     'Activity Label',\n",
    "#     'X',\n",
    "#     'Y',\n",
    "#     'Z'\n",
    "# ])\n",
    "\n",
    "# df_accel = df_all_sensors.select([\n",
    "#     'Subject-id',\n",
    "#     'Timestamp',\n",
    "#     'Activity Label',\n",
    "#     pl.col('X_right').alias('X'),\n",
    "#     pl.col('Y_right').alias('Y'),\n",
    "#     pl.col('Z_right').alias('Z')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "id": "26a5ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel = convert_timestamp(df_accel)\n",
    "df_gyro = convert_timestamp(df_gyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "id": "3447884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_sensors = df_gyro.join(df_accel, on = ['Subject-id', 'Timestamp', 'Activity Label'], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "id": "e5255e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gyro = df_all_sensors.select(\n",
    "#     pl.col('Subject-id'),\n",
    "#     pl.col('Timestamp'),\n",
    "#     pl.col('Activity Label'),\n",
    "#     pl.col('X'),\n",
    "#     pl.col('Y'),\n",
    "#     pl.col('Z')\n",
    "# )\n",
    "\n",
    "# df_accel = df_all_sensors.select(\n",
    "#     pl.col('Subject-id'),\n",
    "#     pl.col('Timestamp'),\n",
    "#     pl.col('Activity Label'),\n",
    "#     pl.col('X_right').alias('X'),\n",
    "#     pl.col('Y_right').alias('Y'),\n",
    "#     pl.col('Z_right').alias('Z')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "id": "1cf30484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axes_by_activity(df_gyro, df_accel, max_samples_per_activity=1000, figsize=(15, 12)):\n",
    "    \"\"\"\n",
    "    Visualiza los ejes X, Y, Z del giroscopio y acelerómetro por cada actividad\n",
    "    \"\"\"\n",
    "    # Obtener actividades únicas\n",
    "    activities = df_gyro['Activity Label'].unique().tolist()\n",
    "    n_activities = len(activities)\n",
    "    \n",
    "    # Crear subplots\n",
    "    fig, axes = plt.subplots(n_activities, 2, figsize=figsize)\n",
    "    if n_activities == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, activity in enumerate(activities):\n",
    "        # Filtrar datos por actividad\n",
    "        gyro_activity = df_gyro[df_gyro['Activity Label'] == activity]\n",
    "        accel_activity =  df_accel[df_accel['Activity Label'] == activity]\n",
    "        \n",
    "        # Limitar muestras para mejor visualización\n",
    "        if len(gyro_activity) > max_samples_per_activity:\n",
    "            gyro_activity = gyro_activity.head(max_samples_per_activity)\n",
    "        if len(accel_activity) > max_samples_per_activity:\n",
    "            accel_activity = accel_activity.head(max_samples_per_activity)\n",
    "        \n",
    "        # Convertir a pandas para plotting\n",
    "        gyro_pd = gyro_activity.copy()\n",
    "        accel_pd = accel_activity.copy()\n",
    "        \n",
    "        # Plot Giroscopio\n",
    "        axes[i, 0].plot(gyro_pd['X'], label='X', alpha=0.7, linewidth=1)\n",
    "        axes[i, 0].plot(gyro_pd['Y'], label='Y', alpha=0.7, linewidth=1)\n",
    "        axes[i, 0].plot(gyro_pd['Z'], label='Z', alpha=0.7, linewidth=1)\n",
    "        axes[i, 0].set_title(f'Giroscopio - {activity}')\n",
    "        axes[i, 0].set_ylabel('Valor')\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot Acelerómetro\n",
    "        axes[i, 1].plot(accel_pd['X'], label='X', alpha=0.7, linewidth=1)\n",
    "        axes[i, 1].plot(accel_pd['Y'], label='Y', alpha=0.7, linewidth=1)\n",
    "        axes[i, 1].plot(accel_pd['Z'], label='Z', alpha=0.7, linewidth=1)\n",
    "        axes[i, 1].set_title(f'Acelerómetro - {activity}')\n",
    "        axes[i, 1].set_ylabel('Valor')\n",
    "        axes[i, 1].legend()\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Añadir xlabel solo en la última fila\n",
    "        if i == n_activities - 1:\n",
    "            axes[i, 0].set_xlabel('Muestra')\n",
    "            axes[i, 1].set_xlabel('Muestra')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar visualización\n",
    "# plot_axes_by_activity(df_gyro.to_pandas(), df_accel.to_pandas(), max_samples_per_activity = 2000,  figsize = (20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "id": "62ad9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_gyro   = create_feature_windows(df_gyro, window_seconds = 5, overlap_percent=50, sampling_rate = 20)\n",
    "features_accel  = create_feature_windows(df_accel, window_seconds = 5, overlap_percent=50, sampling_rate = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "id": "a90de002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dic = adaptive_transfer_learning_cnn_lstm(\n",
    "#     base_model_path = r'F:\\UPC\\Tesis\\HARbit-Model\\src\\models\\version\\sensors\\accel_cnn-lstm_wisdm_91_cluster_user.h5', \n",
    "#     target_X = features_accel.drop(columns = ['Subject-id', 'Activity Label']),\n",
    "#     target_y = features_accel['Activity Label'],\n",
    "#     target_le = ,\n",
    "#     source_X = None,\n",
    "#     source_y = None, \n",
    "#     source_le = None,\n",
    "#     validation_split = 0.2,\n",
    "#     progressive_training = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "id": "bab1d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_combined = pd.merge(\n",
    "#     features_gyro,\n",
    "#     features_accel, \n",
    "#     on=['Subject-id', 'Activity Label', 'window_start', 'window_end', 'sample_count'], \n",
    "#     how=\"inner\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "id": "051b0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "id": "1e753555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Secuencias creadas: (19, 8, 68)\n",
      "  Num features: 68\n",
      "  Clases: ['Type']\n",
      "Forma de X (secuencias): (19, 8, 68)\n",
      "Forma de y: (19,)\n",
      "Actividades únicas: [0]\n"
     ]
    }
   ],
   "source": [
    "X, y, _, le = prepare_features_for_cnn_lstm_sequences(\n",
    "    features_accel, \n",
    "    group_size=8, \n",
    "    step_size=1\n",
    ")\n",
    "\n",
    "print(f\"Forma de X (secuencias): {X.shape}\")   # (N, group_size, features)\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"Actividades únicas: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "id": "5298b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "id": "d2086faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model(r'F:\\UPC\\Tesis\\HARbit-Model\\src\\models\\version\\sensors\\accel_cnn-lstm_wisdm_91_cluster_user.h5')\n",
    "label_encoder = joblib.load(r'F:\\UPC\\Tesis\\HARbit-Model\\src\\models\\meta\\sensors\\accel_cnn-lstm_wisdm_91_cluster_user.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "id": "7f0167f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "id": "9208b133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step\n",
      "✅ Accuracy filtrado (solo clases presentes en el dataset): 0.0000\n",
      "\n",
      "Pérdida en test: 6.2503\n",
      "Precisión en test: 0.0000\n",
      "\n",
      "==================================================\n",
      "REPORTE DE CLASIFICACIÓN\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Type     0.0000    0.0000    0.0000      19.0\n",
      "\n",
      "   micro avg     0.0000    0.0000    0.0000      19.0\n",
      "   macro avg     0.0000    0.0000    0.0000      19.0\n",
      "weighted avg     0.0000    0.0000    0.0000      19.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\forecasting\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\HP\\miniconda3\\envs\\forecasting\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\HP\\miniconda3\\envs\\forecasting\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\HP\\miniconda3\\envs\\forecasting\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluando modelo...\")\n",
    "# Predicciones\n",
    "y_pred = model.predict(X)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Obtener las clases que realmente existen en este conjunto de datos\n",
    "unique_classes = np.unique(y)\n",
    "\n",
    "# Calcular accuracy SOLO en esas clases\n",
    "mask = np.isin(y, unique_classes)\n",
    "filtered_accuracy = accuracy_score(y[mask], y_pred_classes[mask])\n",
    "\n",
    "print(f\"✅ Accuracy filtrado (solo clases presentes en el dataset): {filtered_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Métricas básicas\n",
    "test_loss, test_accuracy = model.evaluate(X, y, verbose=0)\n",
    "print(f\"\\nPérdida en test: {test_loss:.4f}\")\n",
    "print(f\"Precisión en test: {test_accuracy:.4f}\")\n",
    "\n",
    "# Reporte de clasificación detallado\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REPORTE DE CLASIFICACIÓN\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(\n",
    "    y, \n",
    "    y_pred_classes, \n",
    "    labels=unique_classes,               # solo las clases presentes\n",
    "    target_names=label_encoder.classes_[unique_classes],\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "id": "f9d79fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fq = {label_encoder.classes_[yb]: 0 for yb in np.unique(y_pred_classes)}\n",
    "\n",
    "for lb in y_pred_classes:\n",
    "    fq[label_encoder.classes_[lb]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "id": "d4092eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eat': 6, 'Others': 4, 'Sit': 1, 'Stand': 2, 'Walk': 6}"
      ]
     },
     "execution_count": 1545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "id": "62dfa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer.ViterbiLiteDecoder import ViterbiLiteDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "id": "f424f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_viterbi_to_har_results(model, X_test, y_test, label_encoder, \n",
    "                                visualization=True, save_results=True):\n",
    "    \"\"\"\n",
    "    Aplica Viterbi-lite a resultados de HAR\n",
    "    \"\"\"\n",
    "    print(\"🚀 APLICANDO VITERBI-LITE A RESULTADOS HAR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Obtener probabilidades del modelo\n",
    "    print(\"🔍 Extrayendo probabilidades...\")\n",
    "    probabilities = model.predict(X_test)\n",
    "    original_predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    # 2. Crear decodificador con clases específicas\n",
    "    classes = label_encoder.classes_\n",
    "    decoder = ViterbiLiteDecoder(\n",
    "        classes=classes,\n",
    "        transition_penalty=2.5,  # Ajustar según tus necesidades\n",
    "        self_bonus=1.8,\n",
    "        min_duration={\n",
    "            'Walk': 3,       # 15 segundos\n",
    "            'Sit': 4,        # 20 segundos  \n",
    "            'Stand': 2,      # 10 segundos\n",
    "            'Type': 6,       # 30 segundos\n",
    "            'Eat': 4,        # 20 segundos\n",
    "            'Write': 4,      # 20 segundos\n",
    "            'Workouts': 8,   # 40 segundos\n",
    "            'Others': 1      # Sin restricción\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 3. Decodificar secuencia\n",
    "    print(\"🧠 Decodificando secuencia con Viterbi...\")\n",
    "    decoded_sequence, viterbi_scores = decoder.decode_complete_pipeline(\n",
    "        probabilities=probabilities,\n",
    "        apply_duration_constraints=True\n",
    "    )\n",
    "    \n",
    "    # 4. Evaluar mejoras\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    \n",
    "    original_accuracy = accuracy_score(y_test, original_predictions)\n",
    "    viterbi_accuracy = accuracy_score(y_test, decoded_sequence)\n",
    "    \n",
    "    print(f\"\\n📊 RESULTADOS:\")\n",
    "    print(f\"🔵 Accuracy original: {original_accuracy:.4f}\")\n",
    "    print(f\"🟢 Accuracy con Viterbi: {viterbi_accuracy:.4f}\")\n",
    "    print(f\"📈 Mejora: {viterbi_accuracy - original_accuracy:.4f}\")\n",
    "    \n",
    "    # 5. Análisis de cambios\n",
    "    changes = np.sum(original_predictions != decoded_sequence)\n",
    "    print(f\"🔄 Frames modificados: {changes}/{len(y_test)} ({100*changes/len(y_test):.1f}%)\")\n",
    "    \n",
    "    # 7. Visualización\n",
    "    # if visualization:\n",
    "    #     decoder.visualize_decoding(\n",
    "    #         original_sequence=original_predictions,\n",
    "    #         decoded_sequence=decoded_sequence,\n",
    "    #         probabilities=probabilities,\n",
    "    #         save_path='viterbi_analysis.png' if save_results else None\n",
    "    #     )\n",
    "    \n",
    "    # 8. Guardar resultados\n",
    "    # if save_results:\n",
    "    #     import joblib\n",
    "    #     results = {\n",
    "    #         'original_predictions': original_predictions,\n",
    "    #         'viterbi_predictions': decoded_sequence,\n",
    "    #         'probabilities': probabilities,\n",
    "    #         'viterbi_scores': viterbi_scores,\n",
    "    #         'accuracy_improvement': viterbi_accuracy - original_accuracy,\n",
    "    #         'decoder_config': {\n",
    "    #             'classes': classes.tolist(),\n",
    "    #             'transition_penalty': decoder.transition_penalty,\n",
    "    #             'self_bonus': decoder.self_bonus,\n",
    "    #             'min_duration': decoder.min_duration\n",
    "    #         }\n",
    "    #     }\n",
    "        \n",
    "    #     joblib.dump(results, 'viterbi_har_results.joblib')\n",
    "    #     print(\"💾 Resultados guardados en 'viterbi_har_results.joblib'\")\n",
    "    \n",
    "    return {\n",
    "        'decoder': decoder,\n",
    "        'original_predictions': original_predictions,\n",
    "        'viterbi_predictions': decoded_sequence,\n",
    "        'original_accuracy': original_accuracy,\n",
    "        'viterbi_accuracy': viterbi_accuracy,\n",
    "        'improvement': viterbi_accuracy - original_accuracy\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso con tu modelo\n",
    "def run_viterbi_on_trained_model():\n",
    "    \"\"\"\n",
    "    Ejecuta Viterbi en tu modelo ya entrenado\n",
    "    \"\"\"\n",
    "    # Cargar tu modelo y datos\n",
    "    from tensorflow import keras\n",
    "    import joblib\n",
    "    \n",
    "    model = keras.models.load_model(r'F:\\UPC\\Tesis\\HARbit-Model\\src\\models\\version\\sensors\\accel_cnn-lstm_wisdm_91_cluster_user.h5')\n",
    "    label_encoder = joblib.load(r'F:\\UPC\\Tesis\\HARbit-Model\\src\\models\\meta\\sensors\\accel_cnn-lstm_wisdm_91_cluster_user.joblib')\n",
    "    \n",
    "    # X_test, y_test = cargar_tus_datos_de_test()\n",
    "    \n",
    "    # Aplicar Viterbi\n",
    "    results = apply_viterbi_to_har_results(\n",
    "        model=model,\n",
    "        X_test=X,\n",
    "        y_test=y,\n",
    "        label_encoder=label_encoder,\n",
    "        visualization=True,\n",
    "        save_results=True\n",
    "    )\n",
    "    \n",
    "    print(f\"🎉 Viterbi completado con mejora de {results['improvement']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "id": "c277198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 APLICANDO VITERBI-LITE A RESULTADOS HAR\n",
      "============================================================\n",
      "🔍 Extrayendo probabilidades...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 530ms/step\n",
      "🧠 Decodificando secuencia con Viterbi...\n",
      "🧠 Decodificando con probabilidades (Viterbi completo)\n",
      "⏱️ Aplicando restricciones de duración mínima\n",
      "\n",
      "📊 RESULTADOS:\n",
      "🔵 Accuracy original: 0.0000\n",
      "🟢 Accuracy con Viterbi: 0.0000\n",
      "📈 Mejora: 0.0000\n",
      "🔄 Frames modificados: 5/19 (26.3%)\n",
      "🎉 Viterbi completado con mejora de 0.0000\n"
     ]
    }
   ],
   "source": [
    "rs = run_viterbi_on_trained_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "id": "34865d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados[target].append((rs['original_accuracy'], rs['viterbi_accuracy'], rs['improvement']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
